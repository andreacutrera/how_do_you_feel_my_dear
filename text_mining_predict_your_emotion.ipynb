{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc79a1c",
   "metadata": {},
   "source": [
    "## Try to write down some texts and get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fc0f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/24/nqjd68556n7_t8xcfyl1lvpc0000gn/T/ipykernel_60108/2035162933.py:18: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#!pip install neattext\n",
    "import neattext.functions as nfx\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, Dropout, Input, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a1ace",
   "metadata": {},
   "source": [
    "## Load the model trained for the emotions prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2a5cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 22:44:21.705123: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 200)           2180000   \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 50, 200)           0         \n",
      "                                                                 \n",
      " bidirectional_27 (Bidirecti  (None, 50, 100)          100400    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 50, 100)           0         \n",
      "                                                                 \n",
      " bidirectional_28 (Bidirecti  (None, 50, 300)          301200    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 50, 300)           0         \n",
      "                                                                 \n",
      " bidirectional_29 (Bidirecti  (None, 100)              140400    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,722,404\n",
      "Trainable params: 542,404\n",
      "Non-trainable params: 2,180,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('how_do_you_feel_my_dear/final_model/model.h5')\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8411a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabetical order\n",
    "# 0: anger\n",
    "# 1: fear\n",
    "# 2: joy\n",
    "# 3: sadness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b24a75",
   "metadata": {},
   "source": [
    "## Load the tokenizer on which we trained our model\n",
    "If you use another tokenizer it is not going to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2243c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('how_do_you_feel_my_dear/tokenizer/tokenizer.json') as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "tokenizer = tokenizer_from_json(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c7ea636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_emoji_output(text):\n",
    "    return re.sub(\":\", \" \", text)\n",
    "\n",
    "def strip_lowercase(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "# tokenize\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "# function which cleans texts\n",
    "def clean_text(data):\n",
    "    data['clean_text'] = data['text'].apply(nfx.remove_emails)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_numbers)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_urls)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_userhandles)\n",
    "    data['clean_text'] = data['clean_text'].apply(demoji.replace_with_desc)\n",
    "    data['clean_text'] = data['clean_text'].apply(clean_emoji_output)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_special_characters)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_bad_quotes)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_html_tags)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_punctuations)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_stopwords)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_multiple_spaces)\n",
    "    data['clean_text'] = data['clean_text'].apply(strip_lowercase)\n",
    "    \n",
    "    data['tokenize'] = data.clean_text.str.lower().apply(tt.tokenize)\n",
    "    data['tokenize_lemmatized'] = data['tokenize'].apply(lemmatize_text)\n",
    "    \n",
    "    # detokenize\n",
    "    data['final_text'] = data.tokenize_lemmatized.apply(TreebankWordDetokenizer().detokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb8bbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello to EVERYbody, I'm here because I'm happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So sad, what Happened yesterday, many #deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@andrea I want to kill you, no matter how far ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@mathias that way scares me a lot°çççéé</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0     hello to EVERYbody, I'm here because I'm happy\n",
       "1      So sad, what Happened yesterday, many #deaths\n",
       "2  @andrea I want to kill you, no matter how far ...\n",
       "3            @mathias that way scares me a lot°çççéé"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringhe = [\"hello to EVERYbody, I'm here because I'm happy\",\n",
    "            \"So sad, what Happened yesterday, many #deaths\",\n",
    "            \"@andrea I want to kill you, no matter how far you are, I will find you\",\n",
    "            \"@mathias that way scares me a lot°çççéé\"]\n",
    "\n",
    "data = pd.DataFrame({\"text\": stringhe})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d154749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>tokenize_lemmatized</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello to EVERYbody, I'm here because I'm happy</td>\n",
       "      <td>hello everybody im im happy</td>\n",
       "      <td>[hello, everybody, im, im, happy]</td>\n",
       "      <td>[hello, everybody, im, im, happy]</td>\n",
       "      <td>hello everybody im im happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So sad, what Happened yesterday, many #deaths</td>\n",
       "      <td>sad happened yesterday deaths</td>\n",
       "      <td>[sad, happened, yesterday, deaths]</td>\n",
       "      <td>[sad, happened, yesterday, death]</td>\n",
       "      <td>sad happened yesterday death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@andrea I want to kill you, no matter how far ...</td>\n",
       "      <td>want kill matter far find</td>\n",
       "      <td>[want, kill, matter, far, find]</td>\n",
       "      <td>[want, kill, matter, far, find]</td>\n",
       "      <td>want kill matter far find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@mathias that way scares me a lot°çççéé</td>\n",
       "      <td>way scares lot</td>\n",
       "      <td>[way, scares, lot]</td>\n",
       "      <td>[way, scare, lot]</td>\n",
       "      <td>way scare lot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0     hello to EVERYbody, I'm here because I'm happy   \n",
       "1      So sad, what Happened yesterday, many #deaths   \n",
       "2  @andrea I want to kill you, no matter how far ...   \n",
       "3            @mathias that way scares me a lot°çççéé   \n",
       "\n",
       "                      clean_text                            tokenize  \\\n",
       "0    hello everybody im im happy   [hello, everybody, im, im, happy]   \n",
       "1  sad happened yesterday deaths  [sad, happened, yesterday, deaths]   \n",
       "2      want kill matter far find     [want, kill, matter, far, find]   \n",
       "3                 way scares lot                  [way, scares, lot]   \n",
       "\n",
       "                 tokenize_lemmatized                    final_text  \n",
       "0  [hello, everybody, im, im, happy]   hello everybody im im happy  \n",
       "1  [sad, happened, yesterday, death]  sad happened yesterday death  \n",
       "2    [want, kill, matter, far, find]     want kill matter far find  \n",
       "3                  [way, scare, lot]                 way scare lot  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean and shuffle\n",
    "\n",
    "clean_text(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3512e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 995, 1332,    2,    2,   15,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [  31,  570,  468,  491,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [  16,  342,  467,  310,   98,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert texts into some numeric sequences and make the length of all numeric sequences equal \n",
    "\n",
    "X_seq = tokenizer.texts_to_sequences(data.final_text) \n",
    "X_pad = pad_sequences(X_seq, maxlen = 50, padding = 'post') \n",
    "\n",
    "X_pad = np.array(X_pad)\n",
    "X_pad[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c84fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     hello everybody im im happy\n",
       "1    sad happened yesterday death\n",
       "2       want kill matter far find\n",
       "Name: final_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.final_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e23d9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00932676, 0.00625352, 0.97586864, 0.00855107],\n",
       "       [0.02307396, 0.13095938, 0.11437412, 0.7315926 ],\n",
       "       [0.9695867 , 0.01440939, 0.00753933, 0.00846465],\n",
       "       [0.00296482, 0.9935203 , 0.00148988, 0.00202487]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ca1f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>tokenize_lemmatized</th>\n",
       "      <th>final_text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello to EVERYbody, I'm here because I'm happy</td>\n",
       "      <td>hello everybody im im happy</td>\n",
       "      <td>[hello, everybody, im, im, happy]</td>\n",
       "      <td>[hello, everybody, im, im, happy]</td>\n",
       "      <td>hello everybody im im happy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So sad, what Happened yesterday, many #deaths</td>\n",
       "      <td>sad happened yesterday deaths</td>\n",
       "      <td>[sad, happened, yesterday, deaths]</td>\n",
       "      <td>[sad, happened, yesterday, death]</td>\n",
       "      <td>sad happened yesterday death</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@andrea I want to kill you, no matter how far ...</td>\n",
       "      <td>want kill matter far find</td>\n",
       "      <td>[want, kill, matter, far, find]</td>\n",
       "      <td>[want, kill, matter, far, find]</td>\n",
       "      <td>want kill matter far find</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@mathias that way scares me a lot°çççéé</td>\n",
       "      <td>way scares lot</td>\n",
       "      <td>[way, scares, lot]</td>\n",
       "      <td>[way, scare, lot]</td>\n",
       "      <td>way scare lot</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0     hello to EVERYbody, I'm here because I'm happy   \n",
       "1      So sad, what Happened yesterday, many #deaths   \n",
       "2  @andrea I want to kill you, no matter how far ...   \n",
       "3            @mathias that way scares me a lot°çççéé   \n",
       "\n",
       "                      clean_text                            tokenize  \\\n",
       "0    hello everybody im im happy   [hello, everybody, im, im, happy]   \n",
       "1  sad happened yesterday deaths  [sad, happened, yesterday, deaths]   \n",
       "2      want kill matter far find     [want, kill, matter, far, find]   \n",
       "3                 way scares lot                  [way, scares, lot]   \n",
       "\n",
       "                 tokenize_lemmatized                    final_text prediction  \n",
       "0  [hello, everybody, im, im, happy]   hello everybody im im happy        joy  \n",
       "1  [sad, happened, yesterday, death]  sad happened yesterday death    sadness  \n",
       "2    [want, kill, matter, far, find]     want kill matter far find      anger  \n",
       "3                  [way, scare, lot]                 way scare lot       fear  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['prediction'] = np.argmax(model.predict(X_pad), axis=1)\n",
    "\n",
    "# alphabetical order\n",
    "# 0: anger\n",
    "# 1: fear\n",
    "# 2: joy\n",
    "# 3: sadness\n",
    "\n",
    "code_to_strings = {0: \"anger\",\n",
    "                   1: \"fear\",\n",
    "                   2: \"joy\",\n",
    "                   3: \"sadness\"}\n",
    "\n",
    "data[\"prediction\"] = data[\"prediction\"].map(code_to_strings)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b981882b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.prediction.value_counts().sort_index(ascending=True).plot(kind='barh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
