{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc79a1c",
   "metadata": {},
   "source": [
    "## Try to write down some texts and get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fc0f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/24/nqjd68556n7_t8xcfyl1lvpc0000gn/T/ipykernel_11332/2035162933.py:18: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#!pip install neattext\n",
    "import neattext.functions as nfx\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, Dropout, Input, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a1ace",
   "metadata": {},
   "source": [
    "## Load the model trained for the emotions prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2a5cce",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at how_do_you_feel_my_dear/final_model/model.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/nqjd68556n7_t8xcfyl1lvpc0000gn/T/ipykernel_11332/1168146072.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'how_do_you_feel_my_dear/final_model/model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# summarize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             raise ImportError(\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at how_do_you_feel_my_dear/final_model/model.h5"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('how_do_you_feel_my_dear/final_model/model.h5')\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabetical order\n",
    "# 0: anger\n",
    "# 1: fear\n",
    "# 2: joy\n",
    "# 3: sadness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b24a75",
   "metadata": {},
   "source": [
    "## Load the tokenizer on which we trained our model\n",
    "If you use another tokenizer it is not going to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2243c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('how_do_you_feel_my_dear/tokenizer/tokenizer.json') as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "tokenizer = tokenizer_from_json(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7ea636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_emoji_output(text):\n",
    "    return re.sub(\":\", \" \", text)\n",
    "\n",
    "def strip_lowercase(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "# tokenize\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "# function which cleans texts\n",
    "def clean_text(data):\n",
    "    data['clean_text'] = data['text'].apply(nfx.remove_emails)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_numbers)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_urls)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_userhandles)\n",
    "    data['clean_text'] = data['clean_text'].apply(demoji.replace_with_desc)\n",
    "    data['clean_text'] = data['clean_text'].apply(clean_emoji_output)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_special_characters)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_bad_quotes)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_html_tags)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_punctuations)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_stopwords)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_multiple_spaces)\n",
    "    data['clean_text'] = data['clean_text'].apply(strip_lowercase)\n",
    "    \n",
    "    data['tokenize'] = data.clean_text.str.lower().apply(tt.tokenize)\n",
    "    data['tokenize_lemmatized'] = data['tokenize'].apply(lemmatize_text)\n",
    "    \n",
    "    # detokenize\n",
    "    data['final_text'] = data.tokenize_lemmatized.apply(TreebankWordDetokenizer().detokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb8bbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@myusername Today I'm VEry happY because my we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So sad, what Happened yesterday, many #deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@andrea I want to kill you, no matter how far ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@mathias that way scares me a lot¬∞√ß√ß√ß√©√©</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @myusername Today I'm VEry happY because my we...\n",
       "1      So sad, what Happened yesterday, many #deaths\n",
       "2  @andrea I want to kill you, no matter how far ...\n",
       "3            @mathias that way scares me a lot¬∞√ß√ß√ß√©√©"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringhe = [\"@myusername Today I'm VEry happY because my website  333876 https://mysite.it HAS been 'deployed''''.          üòçüòÅüòÅ contact me if something √ß√ß√ß√ß¬∞¬∞¬∞ need to be\"\" fixed.....'': usr@gmail.com\",\n",
    "            \"So sad, what Happened yesterday, many #deaths\",\n",
    "            \"@andrea I want to kill you, no matter how far you are, I will find you\",\n",
    "            \"@mathias that way scares me a lot¬∞√ß√ß√ß√©√©\"]\n",
    "\n",
    "data = pd.DataFrame({\"text\": stringhe})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d154749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>tokenize_lemmatized</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@myusername Today I'm VEry happY because my we...</td>\n",
       "      <td>today im happy website deployed smiling face h...</td>\n",
       "      <td>[today, im, happy, website, deployed, smiling,...</td>\n",
       "      <td>[today, im, happy, website, deployed, smiling,...</td>\n",
       "      <td>today im happy website deployed smiling face h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So sad, what Happened yesterday, many #deaths</td>\n",
       "      <td>sad happened yesterday deaths</td>\n",
       "      <td>[sad, happened, yesterday, deaths]</td>\n",
       "      <td>[sad, happened, yesterday, death]</td>\n",
       "      <td>sad happened yesterday death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@andrea I want to kill you, no matter how far ...</td>\n",
       "      <td>want kill matter far find</td>\n",
       "      <td>[want, kill, matter, far, find]</td>\n",
       "      <td>[want, kill, matter, far, find]</td>\n",
       "      <td>want kill matter far find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@mathias that way scares me a lot¬∞√ß√ß√ß√©√©</td>\n",
       "      <td>way scares lot</td>\n",
       "      <td>[way, scares, lot]</td>\n",
       "      <td>[way, scare, lot]</td>\n",
       "      <td>way scare lot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @myusername Today I'm VEry happY because my we...   \n",
       "1      So sad, what Happened yesterday, many #deaths   \n",
       "2  @andrea I want to kill you, no matter how far ...   \n",
       "3            @mathias that way scares me a lot¬∞√ß√ß√ß√©√©   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  today im happy website deployed smiling face h...   \n",
       "1                      sad happened yesterday deaths   \n",
       "2                          want kill matter far find   \n",
       "3                                     way scares lot   \n",
       "\n",
       "                                            tokenize  \\\n",
       "0  [today, im, happy, website, deployed, smiling,...   \n",
       "1                 [sad, happened, yesterday, deaths]   \n",
       "2                    [want, kill, matter, far, find]   \n",
       "3                                 [way, scares, lot]   \n",
       "\n",
       "                                 tokenize_lemmatized  \\\n",
       "0  [today, im, happy, website, deployed, smiling,...   \n",
       "1                  [sad, happened, yesterday, death]   \n",
       "2                    [want, kill, matter, far, find]   \n",
       "3                                  [way, scare, lot]   \n",
       "\n",
       "                                          final_text  \n",
       "0  today im happy website deployed smiling face h...  \n",
       "1                       sad happened yesterday death  \n",
       "2                          want kill matter far find  \n",
       "3                                      way scare lot  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean and shuffle\n",
    "\n",
    "clean_text(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdcecb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['today im happy website deployed smiling face hearteyes beaming face smiling eye beaming face smiling eye contact need fixed',\n",
       "       'sad happened yesterday death', 'want kill matter far find',\n",
       "       'way scare lot'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.final_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3512e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 995, 1332,    2,    2,   15,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [  31,  570,  468,  491,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [  16,  342,  467,  310,   98,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert texts into some numeric sequences and make the length of all numeric sequences equal \n",
    "\n",
    "X_seq = tokenizer.texts_to_sequences(data.final_text) \n",
    "X_pad = pad_sequences(X_seq, maxlen = 50, padding = 'post') \n",
    "\n",
    "X_pad = np.array(X_pad)\n",
    "X_pad[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c84fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     hello everybody im im happy\n",
       "1    sad happened yesterday death\n",
       "2       want kill matter far find\n",
       "Name: final_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.final_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e23d9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00932676, 0.00625352, 0.97586864, 0.00855107],\n",
       "       [0.02307396, 0.13095938, 0.11437412, 0.7315926 ],\n",
       "       [0.9695867 , 0.01440939, 0.00753933, 0.00846465],\n",
       "       [0.00296482, 0.9935203 , 0.00148988, 0.00202487]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ca1f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>tokenize_lemmatized</th>\n",
       "      <th>final_text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello to EVERYbody, I'm here because I'm happy</td>\n",
       "      <td>hello everybody im im happy</td>\n",
       "      <td>[hello, everybody, im, im, happy]</td>\n",
       "      <td>[hello, everybody, im, im, happy]</td>\n",
       "      <td>hello everybody im im happy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So sad, what Happened yesterday, many #deaths</td>\n",
       "      <td>sad happened yesterday deaths</td>\n",
       "      <td>[sad, happened, yesterday, deaths]</td>\n",
       "      <td>[sad, happened, yesterday, death]</td>\n",
       "      <td>sad happened yesterday death</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@andrea I want to kill you, no matter how far ...</td>\n",
       "      <td>want kill matter far find</td>\n",
       "      <td>[want, kill, matter, far, find]</td>\n",
       "      <td>[want, kill, matter, far, find]</td>\n",
       "      <td>want kill matter far find</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@mathias that way scares me a lot¬∞√ß√ß√ß√©√©</td>\n",
       "      <td>way scares lot</td>\n",
       "      <td>[way, scares, lot]</td>\n",
       "      <td>[way, scare, lot]</td>\n",
       "      <td>way scare lot</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0     hello to EVERYbody, I'm here because I'm happy   \n",
       "1      So sad, what Happened yesterday, many #deaths   \n",
       "2  @andrea I want to kill you, no matter how far ...   \n",
       "3            @mathias that way scares me a lot¬∞√ß√ß√ß√©√©   \n",
       "\n",
       "                      clean_text                            tokenize  \\\n",
       "0    hello everybody im im happy   [hello, everybody, im, im, happy]   \n",
       "1  sad happened yesterday deaths  [sad, happened, yesterday, deaths]   \n",
       "2      want kill matter far find     [want, kill, matter, far, find]   \n",
       "3                 way scares lot                  [way, scares, lot]   \n",
       "\n",
       "                 tokenize_lemmatized                    final_text prediction  \n",
       "0  [hello, everybody, im, im, happy]   hello everybody im im happy        joy  \n",
       "1  [sad, happened, yesterday, death]  sad happened yesterday death    sadness  \n",
       "2    [want, kill, matter, far, find]     want kill matter far find      anger  \n",
       "3                  [way, scare, lot]                 way scare lot       fear  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['prediction'] = np.argmax(model.predict(X_pad), axis=1)\n",
    "\n",
    "# alphabetical order\n",
    "# 0: anger\n",
    "# 1: fear\n",
    "# 2: joy\n",
    "# 3: sadness\n",
    "\n",
    "code_to_strings = {0: \"anger\",\n",
    "                   1: \"fear\",\n",
    "                   2: \"joy\",\n",
    "                   3: \"sadness\"}\n",
    "\n",
    "data[\"prediction\"] = data[\"prediction\"].map(code_to_strings)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b981882b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.prediction.value_counts().sort_index(ascending=True).plot(kind='barh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
