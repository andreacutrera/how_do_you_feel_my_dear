{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276fbc01",
   "metadata": {
    "id": "2db7863b"
   },
   "source": [
    "# How do you feel my dear? (P8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa40ff8",
   "metadata": {},
   "source": [
    "## Andrea Pio Cutrera - 965591\n",
    "### Universit√† degli Studi di Milano - _Data Science and Economics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1ff10",
   "metadata": {},
   "source": [
    "**Text Mining and Sentiment Analysis**\n",
    "\n",
    "Recently, **emotion detection in text** has received attention in the literature on **sentiment analysis**. Detecting emotions is important for studying human communication in different domains, including fictional scripts for TV series and movies. The project aims at studying fictional scripts of several movies and TV series under the emotional profile. In particular, the task of the project is threefold:\n",
    "\n",
    "1. Create a **model to predict emotions in text** using available datasets as EmoBank or WASSA-2017 or Emotion Detection from Text as training sets (see below);\n",
    "2. Emotions may be represented either as **categorical classes** or in a continuous space such as Valence-Arousal-Dominance (see for example Warriner, A. B., Kuperman, V., & Brysbaert, M. (2013). Norms of valence, arousal, and dominance for 13,915 English lemmas. Behavior research methods, 45(4), 1191-1207.)\n",
    "3. Exploit the model to **study an emotional profile** of the **main characters** in **one of the movies** included in the Cornell Movie--Dialogs Corpus;\n",
    "4. Study how this **emotional profile changes in time along** the evolution of the movie story and how it is affected by the various relations among the different characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd913b95",
   "metadata": {},
   "source": [
    "### Import all the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b148f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#!pip install neattext\n",
    "import neattext.functions as nfx\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, Dropout, Input, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
    "from kerastuner.tuners import RandomSearch, Hyperband\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecc97bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/andreacutrera/Desktop'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_directory = !pwd                 # get present working directory\n",
    "working_directory = working_directory[0] # get the full string \n",
    "working_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49567a89",
   "metadata": {
    "id": "49567a89"
   },
   "source": [
    "### Load the model which has been trained on Dataset: WASSA-2017 Shared Task on Emotion Intensity (EmoInt)\n",
    "link: http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdaf31bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 18:16:53.922072: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 200)           2180000   \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 50, 200)           0         \n",
      "                                                                 \n",
      " bidirectional_27 (Bidirecti  (None, 50, 100)          100400    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 50, 100)           0         \n",
      "                                                                 \n",
      " bidirectional_28 (Bidirecti  (None, 50, 300)          301200    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 50, 300)           0         \n",
      "                                                                 \n",
      " bidirectional_29 (Bidirecti  (None, 100)              140400    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,722,404\n",
      "Trainable params: 542,404\n",
      "Non-trainable params: 2,180,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('how_do_you_feel_my_dear/final_model/model.h5')\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0175de4",
   "metadata": {
    "id": "a0175de4"
   },
   "outputs": [],
   "source": [
    "# load in variables the data just downloaded\n",
    "DIR = os.getcwd() + \"/how_do_you_feel_my_dear/emotions\"\n",
    "\n",
    "joy_test = pd.read_csv(os.path.join(DIR, \"joy_test\"), sep=\"\\t\", header=None)\n",
    "sadness_test = pd.read_csv(os.path.join(DIR, \"sadness_test\"), sep=\"\\t\", header=None)\n",
    "fear_test = pd.read_csv(os.path.join(DIR, \"fear_test\"), sep=\"\\t\", header=None)\n",
    "anger_test = pd.read_csv(os.path.join(DIR, \"anger_test\"), sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df34285",
   "metadata": {
    "id": "7df34285"
   },
   "outputs": [],
   "source": [
    "# load in variables the data just downloaded\n",
    "\n",
    "joy_train = pd.read_csv(os.path.join(DIR, \"joy_train\"), sep=\"\\t\", header=None)\n",
    "sadness_train = pd.read_csv(os.path.join(DIR, \"sadness_train\"), sep=\"\\t\", header=None)\n",
    "fear_train = pd.read_csv(os.path.join(DIR, \"fear_train\"), sep=\"\\t\", header=None)\n",
    "anger_train = pd.read_csv(os.path.join(DIR, \"anger_train\"), sep=\"\\t\", header=None)\n",
    "\n",
    "joy_val = pd.read_csv(os.path.join(DIR, \"joy_val\"), sep=\"\\t\", header=None)\n",
    "sadness_val = pd.read_csv(os.path.join(DIR, \"sadness_val\"), sep=\"\\t\", header=None)\n",
    "fear_val = pd.read_csv(os.path.join(DIR, \"fear_val\"), sep=\"\\t\", header=None)\n",
    "anger_val = pd.read_csv(os.path.join(DIR, \"anger_val\"), sep=\"\\t\", header=None)\n",
    "\n",
    "joy_test = pd.read_csv(os.path.join(DIR, \"joy_test\"), sep=\"\\t\", header=None)\n",
    "sadness_test = pd.read_csv(os.path.join(DIR, \"sadness_test\"), sep=\"\\t\", header=None)\n",
    "fear_test = pd.read_csv(os.path.join(DIR, \"fear_test\"), sep=\"\\t\", header=None)\n",
    "anger_test = pd.read_csv(os.path.join(DIR, \"anger_test\"), sep=\"\\t\", header=None)\n",
    "\n",
    "# rename columns all in the same way to get homogeneous datasets which could be concatenated\n",
    "\n",
    "joy_train.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'j'}, inplace=True)\n",
    "joy_train['s'] = 0\n",
    "joy_train['a'] = 0\n",
    "joy_train['f'] = 0\n",
    "joy_train = joy_train[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "joy_val.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'j'}, inplace=True)\n",
    "joy_val['s'] = 0\n",
    "joy_val['a'] = 0\n",
    "joy_val['f'] = 0\n",
    "joy_val = joy_val[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "joy_test.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'j'}, inplace=True)\n",
    "joy_test['s'] = 0\n",
    "joy_test['a'] = 0\n",
    "joy_test['f'] = 0\n",
    "joy_test = joy_test[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "sadness_train.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 's'}, inplace=True)\n",
    "sadness_train['j'] = 0\n",
    "sadness_train['a'] = 0\n",
    "sadness_train['f'] = 0\n",
    "sadness_train = sadness_train[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "sadness_val.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 's'}, inplace=True)\n",
    "sadness_val['j'] = 0\n",
    "sadness_val['a'] = 0\n",
    "sadness_val['f'] = 0\n",
    "sadness_val = sadness_val[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "sadness_test.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 's'}, inplace=True)\n",
    "sadness_test['j'] = 0\n",
    "sadness_test['a'] = 0\n",
    "sadness_test['f'] = 0\n",
    "sadness_test = sadness_test[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "anger_train.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'a'}, inplace=True)\n",
    "anger_train['j'] = 0\n",
    "anger_train['s'] = 0\n",
    "anger_train['f'] = 0\n",
    "anger_train = anger_train[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "anger_val.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'a'}, inplace=True)\n",
    "anger_val['j'] = 0\n",
    "anger_val['s'] = 0\n",
    "anger_val['f'] = 0\n",
    "anger_val = anger_val[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "anger_test.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'a'}, inplace=True)\n",
    "anger_test['j'] = 0\n",
    "anger_test['s'] = 0\n",
    "anger_test['f'] = 0\n",
    "anger_test = anger_test[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "fear_train.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'f'}, inplace=True)\n",
    "fear_train['j'] = 0\n",
    "fear_train['s'] = 0\n",
    "fear_train['a'] = 0\n",
    "fear_train = fear_train[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "fear_val.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'f'}, inplace=True)\n",
    "fear_val['j'] = 0\n",
    "fear_val['s'] = 0\n",
    "fear_val['a'] = 0\n",
    "fear_val = fear_val[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "fear_test.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'f'}, inplace=True)\n",
    "fear_test['j'] = 0\n",
    "fear_test['s'] = 0\n",
    "fear_test['a'] = 0\n",
    "fear_test = fear_test[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "# concatenate the datasets in order to get 3 separated datasets: train, test, validation\n",
    "\n",
    "data = pd.concat([joy_train,\n",
    "                  sadness_train,\n",
    "                  fear_train,\n",
    "                  anger_train,\n",
    "                  joy_test,\n",
    "                  sadness_test,\n",
    "                  fear_test,\n",
    "                  anger_test,\n",
    "                  joy_val,\n",
    "                  sadness_val,\n",
    "                  fear_val,\n",
    "                  anger_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a3b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text functions\n",
    "# https://github.com/Jcharis/neattext/blob/master/neattext/functions/functions.py\n",
    "\n",
    "def clean_emoji_output(text):\n",
    "    return re.sub(\":\", \" \", text)\n",
    "\n",
    "def strip_lowercase(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "# tokenize\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "# function which cleans texts\n",
    "def clean_text(data):\n",
    "    data['clean_text'] = data['text'].apply(nfx.remove_emails)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_numbers)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_urls)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_userhandles)\n",
    "    data['clean_text'] = data['clean_text'].apply(demoji.replace_with_desc)\n",
    "    data['clean_text'] = data['clean_text'].apply(clean_emoji_output)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_special_characters)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_bad_quotes)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_html_tags)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_punctuations)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_stopwords)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_multiple_spaces)\n",
    "    data['clean_text'] = data['clean_text'].apply(strip_lowercase)\n",
    "    \n",
    "    data['tokenize'] = data.clean_text.str.lower().apply(tt.tokenize)\n",
    "    data['tokenize_lemmatized'] = data['tokenize'].apply(lemmatize_text)\n",
    "    \n",
    "    # detokenize\n",
    "    data['final_text'] = data.tokenize_lemmatized.apply(TreebankWordDetokenizer().detokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5226dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and shuffle\n",
    "clean_text(data)\n",
    "data = shuffle(data, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1345894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>j</th>\n",
       "      <th>s</th>\n",
       "      <th>f</th>\n",
       "      <th>a</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>tokenize_lemmatized</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>11041</td>\n",
       "      <td>Nothing is more relentless than a dog begging ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479</td>\n",
       "      <td>relentless dog begging food</td>\n",
       "      <td>[relentless, dog, begging, food]</td>\n",
       "      <td>[relentless, dog, begging, food]</td>\n",
       "      <td>relentless dog begging food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>40980</td>\n",
       "      <td>Ok it really just sunk in that I'm seeing the ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>ok sunk im seeing goat hours wow face rolling ...</td>\n",
       "      <td>[ok, sunk, im, seeing, goat, hours, wow, face,...</td>\n",
       "      <td>[ok, sunk, im, seeing, goat, hour, wow, face, ...</td>\n",
       "      <td>ok sunk im seeing goat hour wow face rolling e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>41210</td>\n",
       "      <td>@MariamVeiszadeh #depressing it's so freaking ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>depressing freaking close</td>\n",
       "      <td>[depressing, freaking, close]</td>\n",
       "      <td>[depressing, freaking, close]</td>\n",
       "      <td>depressing freaking close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>41365</td>\n",
       "      <td>I feel like a burden every day that I waste bu...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>feel like burden day waste dont know bc discou...</td>\n",
       "      <td>[feel, like, burden, day, waste, dont, know, b...</td>\n",
       "      <td>[feel, like, burden, day, waste, dont, know, b...</td>\n",
       "      <td>feel like burden day waste dont know bc discou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>31085</td>\n",
       "      <td>@partydelightsUK it's 5679787. Cannot DM you a...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>dm dont follow delight party fail letdown</td>\n",
       "      <td>[dm, dont, follow, delight, party, fail, letdown]</td>\n",
       "      <td>[dm, dont, follow, delight, party, fail, letdown]</td>\n",
       "      <td>dm dont follow delight party fail letdown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text sentiment  \\\n",
       "100  11041  Nothing is more relentless than a dog begging ...     anger   \n",
       "120  40980  Ok it really just sunk in that I'm seeing the ...   sadness   \n",
       "350  41210  @MariamVeiszadeh #depressing it's so freaking ...   sadness   \n",
       "505  41365  I feel like a burden every day that I waste bu...   sadness   \n",
       "183  31085  @partydelightsUK it's 5679787. Cannot DM you a...       joy   \n",
       "\n",
       "         j      s    f      a  \\\n",
       "100  0.000  0.000  0.0  0.479   \n",
       "120  0.000  0.417  0.0  0.000   \n",
       "350  0.000  0.679  0.0  0.000   \n",
       "505  0.000  0.896  0.0  0.000   \n",
       "183  0.164  0.000  0.0  0.000   \n",
       "\n",
       "                                            clean_text  \\\n",
       "100                        relentless dog begging food   \n",
       "120  ok sunk im seeing goat hours wow face rolling ...   \n",
       "350                          depressing freaking close   \n",
       "505  feel like burden day waste dont know bc discou...   \n",
       "183          dm dont follow delight party fail letdown   \n",
       "\n",
       "                                              tokenize  \\\n",
       "100                   [relentless, dog, begging, food]   \n",
       "120  [ok, sunk, im, seeing, goat, hours, wow, face,...   \n",
       "350                      [depressing, freaking, close]   \n",
       "505  [feel, like, burden, day, waste, dont, know, b...   \n",
       "183  [dm, dont, follow, delight, party, fail, letdown]   \n",
       "\n",
       "                                   tokenize_lemmatized  \\\n",
       "100                   [relentless, dog, begging, food]   \n",
       "120  [ok, sunk, im, seeing, goat, hour, wow, face, ...   \n",
       "350                      [depressing, freaking, close]   \n",
       "505  [feel, like, burden, day, waste, dont, know, b...   \n",
       "183  [dm, dont, follow, delight, party, fail, letdown]   \n",
       "\n",
       "                                            final_text  \n",
       "100                        relentless dog begging food  \n",
       "120  ok sunk im seeing goat hour wow face rolling e...  \n",
       "350                          depressing freaking close  \n",
       "505  feel like burden day waste dont know bc discou...  \n",
       "183          dm dont follow delight party fail letdown  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03c188a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation-test split has been already done\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.final_text,\n",
    "                                                    data.sentiment,\n",
    "                                                    test_size = 0.13,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = data.sentiment)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                  y_train,\n",
    "                                                  test_size = 0.14,\n",
    "                                                  random_state = 42,\n",
    "                                                  stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc67aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical encoding of our labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_test_enc = label_encoder.fit_transform(y_test)\n",
    "y_val_enc = label_encoder.fit_transform(y_val)\n",
    "\n",
    "# one-hot-encode them\n",
    "\n",
    "y_train = to_categorical(y_train_enc)\n",
    "y_val = to_categorical(y_val_enc)\n",
    "y_test = to_categorical(y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bca3563",
   "metadata": {
    "id": "0bca3563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10899 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# here we use the Tokenizer of keras, fitting it on all our training and validation data, \n",
    "# getting the sequences and the words with the corresponding index (unique words)\n",
    "\n",
    "num_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=num_words)\n",
    "tokenizer.fit_on_texts(pd.concat([X_train, X_val]))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(pd.concat([X_train, X_val]))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70e334a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # saving \n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)  # loading \n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8396978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "tokenizer_json = tokenizer.to_json() \n",
    "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9911495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "with open('tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "tokenizer = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c434c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts into some numeric sequences and make the length of all numeric sequences equal \n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train) \n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen = 50, padding = 'post') \n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen = 50, padding = 'post')\n",
    "\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen = 50, padding = 'post')\n",
    "\n",
    "X_train_pad = np.array(X_train_pad)\n",
    "X_test_pad = np.array(X_test_pad)\n",
    "X_val_pad = np.array(X_val_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1747841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "205119d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0330c836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.8564013e-03, 3.6703704e-03, 1.9867076e-03, 9.8848653e-01],\n",
       "       [1.2811157e-03, 5.0674111e-04, 6.3612720e-04, 9.9757606e-01],\n",
       "       [7.6428088e-03, 6.5521821e-02, 9.2463005e-01, 2.2053479e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e945e572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.46348777413368225\n",
      "Test accuracy: 0.8409090638160706\n"
     ]
    }
   ],
   "source": [
    "# check external validity on the test set\n",
    "score = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b64a82d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today something went wrong with my exams, I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What a beautiful sunny day, I'm so excited</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Today something went wrong with my exams, I'm ...\n",
       "1         What a beautiful sunny day, I'm so excited"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = [\"Today something went wrong with my exams, I'm so depressed\",\n",
    "           \"What a beautiful sunny day, I'm so excited\"]\n",
    "new_texts = pd.DataFrame({\"text\": strings})\n",
    "\n",
    "new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4a76aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>tokenize_lemmatized</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today something went wrong with my exams, I'm ...</td>\n",
       "      <td>today went wrong exams im depressed</td>\n",
       "      <td>[today, went, wrong, exams, im, depressed]</td>\n",
       "      <td>[today, went, wrong, exam, im, depressed]</td>\n",
       "      <td>today went wrong exam im depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What a beautiful sunny day, I'm so excited</td>\n",
       "      <td>beautiful sunny day im excited</td>\n",
       "      <td>[beautiful, sunny, day, im, excited]</td>\n",
       "      <td>[beautiful, sunny, day, im, excited]</td>\n",
       "      <td>beautiful sunny day im excited</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Today something went wrong with my exams, I'm ...   \n",
       "1         What a beautiful sunny day, I'm so excited   \n",
       "\n",
       "                            clean_text  \\\n",
       "0  today went wrong exams im depressed   \n",
       "1       beautiful sunny day im excited   \n",
       "\n",
       "                                     tokenize  \\\n",
       "0  [today, went, wrong, exams, im, depressed]   \n",
       "1        [beautiful, sunny, day, im, excited]   \n",
       "\n",
       "                         tokenize_lemmatized  \\\n",
       "0  [today, went, wrong, exam, im, depressed]   \n",
       "1       [beautiful, sunny, day, im, excited]   \n",
       "\n",
       "                           final_text  \n",
       "0  today went wrong exam im depressed  \n",
       "1      beautiful sunny day im excited  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(new_texts)\n",
    "new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64cab02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>tokenize_lemmatized</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today something went wrong with my exams, I'm ...</td>\n",
       "      <td>today went wrong exams im depressed</td>\n",
       "      <td>[today, went, wrong, exams, im, depressed]</td>\n",
       "      <td>[today, went, wrong, exam, im, depressed]</td>\n",
       "      <td>today went wrong exam im depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What a beautiful sunny day, I'm so excited</td>\n",
       "      <td>beautiful sunny day im excited</td>\n",
       "      <td>[beautiful, sunny, day, im, excited]</td>\n",
       "      <td>[beautiful, sunny, day, im, excited]</td>\n",
       "      <td>beautiful sunny day im excited</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Today something went wrong with my exams, I'm ...   \n",
       "1         What a beautiful sunny day, I'm so excited   \n",
       "\n",
       "                            clean_text  \\\n",
       "0  today went wrong exams im depressed   \n",
       "1       beautiful sunny day im excited   \n",
       "\n",
       "                                     tokenize  \\\n",
       "0  [today, went, wrong, exams, im, depressed]   \n",
       "1        [beautiful, sunny, day, im, excited]   \n",
       "\n",
       "                         tokenize_lemmatized  \\\n",
       "0  [today, went, wrong, exam, im, depressed]   \n",
       "1       [beautiful, sunny, day, im, excited]   \n",
       "\n",
       "                           final_text  \n",
       "0  today went wrong exam im depressed  \n",
       "1      beautiful sunny day im excited  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts['tokenize'] = new_texts.clean_text.str.lower().apply(tt.tokenize)\n",
    "new_texts['tokenize_lemmatized'] = new_texts['tokenize'].apply(lemmatize_text)\n",
    "new_texts['final_text'] = new_texts.tokenize_lemmatized.apply(TreebankWordDetokenizer().detokenize)\n",
    "new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe980bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  27,  445,  352, 2850,    2, 1257,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [ 244, 2968,    6,    2,  292,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seq = tokenizer.texts_to_sequences(new_texts.final_text)\n",
    "X_pad = pad_sequences(X_seq, maxlen = 50, padding = 'post')\n",
    "\n",
    "X_pad = np.array(X_pad)\n",
    "X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6db99cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(X_pad), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f906c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabetical order\n",
    "# 0: anger\n",
    "# 1: fear\n",
    "# 2: joy\n",
    "# 3: sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef7c318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4cf40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b9920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56b076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3699cf52"
   ],
   "name": "text_mining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
