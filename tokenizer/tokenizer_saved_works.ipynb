{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0efe725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/24/nqjd68556n7_t8xcfyl1lvpc0000gn/T/ipykernel_46891/2035162933.py:18: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#!pip install neattext\n",
    "import neattext.functions as nfx\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, Dropout, Input, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2a5cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 200)           2180000   \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 50, 200)           0         \n",
      "                                                                 \n",
      " bidirectional_27 (Bidirecti  (None, 50, 100)          100400    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 50, 100)           0         \n",
      "                                                                 \n",
      " bidirectional_28 (Bidirecti  (None, 50, 300)          301200    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 50, 300)           0         \n",
      "                                                                 \n",
      " bidirectional_29 (Bidirecti  (None, 100)              140400    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,722,404\n",
      "Trainable params: 542,404\n",
      "Non-trainable params: 2,180,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('how_do_you_feel_my_dear/final_model/model.h5')\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "803a1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in variables the data just downloaded\n",
    "DIR = os.getcwd() + \"/how_do_you_feel_my_dear/emotions\"\n",
    "\n",
    "joy_test = pd.read_csv(os.path.join(DIR, \"joy_test\"), sep=\"\\t\", header=None)\n",
    "sadness_test = pd.read_csv(os.path.join(DIR, \"sadness_test\"), sep=\"\\t\", header=None)\n",
    "fear_test = pd.read_csv(os.path.join(DIR, \"fear_test\"), sep=\"\\t\", header=None)\n",
    "anger_test = pd.read_csv(os.path.join(DIR, \"anger_test\"), sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6e48b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in variables the data just downloaded\n",
    "\n",
    "joy_train = pd.read_csv(os.path.join(DIR, \"joy_train\"), sep=\"\\t\", header=None)\n",
    "sadness_train = pd.read_csv(os.path.join(DIR, \"sadness_train\"), sep=\"\\t\", header=None)\n",
    "fear_train = pd.read_csv(os.path.join(DIR, \"fear_train\"), sep=\"\\t\", header=None)\n",
    "anger_train = pd.read_csv(os.path.join(DIR, \"anger_train\"), sep=\"\\t\", header=None)\n",
    "\n",
    "joy_val = pd.read_csv(os.path.join(DIR, \"joy_val\"), sep=\"\\t\", header=None)\n",
    "sadness_val = pd.read_csv(os.path.join(DIR, \"sadness_val\"), sep=\"\\t\", header=None)\n",
    "fear_val = pd.read_csv(os.path.join(DIR, \"fear_val\"), sep=\"\\t\", header=None)\n",
    "anger_val = pd.read_csv(os.path.join(DIR, \"anger_val\"), sep=\"\\t\", header=None)\n",
    "\n",
    "joy_test = pd.read_csv(os.path.join(DIR, \"joy_test\"), sep=\"\\t\", header=None)\n",
    "sadness_test = pd.read_csv(os.path.join(DIR, \"sadness_test\"), sep=\"\\t\", header=None)\n",
    "fear_test = pd.read_csv(os.path.join(DIR, \"fear_test\"), sep=\"\\t\", header=None)\n",
    "anger_test = pd.read_csv(os.path.join(DIR, \"anger_test\"), sep=\"\\t\", header=None)\n",
    "\n",
    "# rename columns all in the same way to get homogeneous datasets which could be concatenated\n",
    "\n",
    "joy_train.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'j'}, inplace=True)\n",
    "joy_train['s'] = 0\n",
    "joy_train['a'] = 0\n",
    "joy_train['f'] = 0\n",
    "joy_train = joy_train[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "joy_val.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'j'}, inplace=True)\n",
    "joy_val['s'] = 0\n",
    "joy_val['a'] = 0\n",
    "joy_val['f'] = 0\n",
    "joy_val = joy_val[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "joy_test.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'j'}, inplace=True)\n",
    "joy_test['s'] = 0\n",
    "joy_test['a'] = 0\n",
    "joy_test['f'] = 0\n",
    "joy_test = joy_test[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "sadness_train.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 's'}, inplace=True)\n",
    "sadness_train['j'] = 0\n",
    "sadness_train['a'] = 0\n",
    "sadness_train['f'] = 0\n",
    "sadness_train = sadness_train[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "sadness_val.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 's'}, inplace=True)\n",
    "sadness_val['j'] = 0\n",
    "sadness_val['a'] = 0\n",
    "sadness_val['f'] = 0\n",
    "sadness_val = sadness_val[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "sadness_test.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 's'}, inplace=True)\n",
    "sadness_test['j'] = 0\n",
    "sadness_test['a'] = 0\n",
    "sadness_test['f'] = 0\n",
    "sadness_test = sadness_test[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "anger_train.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'a'}, inplace=True)\n",
    "anger_train['j'] = 0\n",
    "anger_train['s'] = 0\n",
    "anger_train['f'] = 0\n",
    "anger_train = anger_train[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "anger_val.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'a'}, inplace=True)\n",
    "anger_val['j'] = 0\n",
    "anger_val['s'] = 0\n",
    "anger_val['f'] = 0\n",
    "anger_val = anger_val[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "anger_test.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'a'}, inplace=True)\n",
    "anger_test['j'] = 0\n",
    "anger_test['s'] = 0\n",
    "anger_test['f'] = 0\n",
    "anger_test = anger_test[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "fear_train.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'f'}, inplace=True)\n",
    "fear_train['j'] = 0\n",
    "fear_train['s'] = 0\n",
    "fear_train['a'] = 0\n",
    "fear_train = fear_train[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "fear_val.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'f'}, inplace=True)\n",
    "fear_val['j'] = 0\n",
    "fear_val['s'] = 0\n",
    "fear_val['a'] = 0\n",
    "fear_val = fear_val[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "fear_test.rename(columns={0: 'id', 1: 'text', 2: 'sentiment', 3: 'f'}, inplace=True)\n",
    "fear_test['j'] = 0\n",
    "fear_test['s'] = 0\n",
    "fear_test['a'] = 0\n",
    "fear_test = fear_test[['id', 'text', 'sentiment', 'j', 's', 'f', 'a']]\n",
    "\n",
    "# concatenate the datasets in order to get 3 separated datasets: train, test, validation\n",
    "\n",
    "data = pd.concat([joy_test,\n",
    "                  sadness_test,\n",
    "                  fear_test,\n",
    "                  anger_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c7ea636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text functions\n",
    "# https://github.com/Jcharis/neattext/blob/master/neattext/functions/functions.py\n",
    "\n",
    "def clean_emoji_output(text):\n",
    "    return re.sub(\":\", \" \", text)\n",
    "\n",
    "def strip_lowercase(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "# tokenize\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "# function which cleans texts\n",
    "def clean_text(data):\n",
    "    data['clean_text'] = data['text'].apply(nfx.remove_emails)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_numbers)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_urls)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_userhandles)\n",
    "    data['clean_text'] = data['clean_text'].apply(demoji.replace_with_desc)\n",
    "    data['clean_text'] = data['clean_text'].apply(clean_emoji_output)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_special_characters)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_bad_quotes)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_html_tags)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_punctuations)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_stopwords)\n",
    "    data['clean_text'] = data['clean_text'].apply(nfx.remove_multiple_spaces)\n",
    "    data['clean_text'] = data['clean_text'].apply(strip_lowercase)\n",
    "    \n",
    "    data['tokenize'] = data.clean_text.str.lower().apply(tt.tokenize)\n",
    "    data['tokenize_lemmatized'] = data['tokenize'].apply(lemmatize_text)\n",
    "    \n",
    "    # detokenize\n",
    "    data['final_text'] = data.tokenize_lemmatized.apply(TreebankWordDetokenizer().detokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d154749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and shuffle\n",
    "clean_text(data)\n",
    "data = shuffle(data, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19b86d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>j</th>\n",
       "      <th>s</th>\n",
       "      <th>f</th>\n",
       "      <th>a</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>tokenize_lemmatized</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>11651</td>\n",
       "      <td>maps by the yeah yeah yeahs came on the radio ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292</td>\n",
       "      <td>maps yeah yeah yeahs came radio today burst tears</td>\n",
       "      <td>[maps, yeah, yeah, yeahs, came, radio, today, ...</td>\n",
       "      <td>[map, yeah, yeah, yeahs, came, radio, today, b...</td>\n",
       "      <td>map yeah yeah yeahs came radio today burst tear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>41241</td>\n",
       "      <td>Dreams dashed and divided like million stars i...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>dreams dashed divided like million stars night...</td>\n",
       "      <td>[dreams, dashed, divided, like, million, stars...</td>\n",
       "      <td>[dream, dashed, divided, like, million, star, ...</td>\n",
       "      <td>dream dashed divided like million star night sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>41276</td>\n",
       "      <td>I did the dishes yesterday, fell asleep, woke ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>dishes yesterday fell asleep woke sink didnt e...</td>\n",
       "      <td>[dishes, yesterday, fell, asleep, woke, sink, ...</td>\n",
       "      <td>[dish, yesterday, fell, asleep, woke, sink, di...</td>\n",
       "      <td>dish yesterday fell asleep woke sink didnt eat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>41440</td>\n",
       "      <td>nooooo. Poor Blue Bell! not again.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nooooo poor blue bell</td>\n",
       "      <td>[nooooo, poor, blue, bell]</td>\n",
       "      <td>[nooooo, poor, blue, bell]</td>\n",
       "      <td>nooooo poor blue bell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>41006</td>\n",
       "      <td>Are you #serious that #fredsirieix lives in Pe...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>fredsirieix lives peckhamsouth londons love gu...</td>\n",
       "      <td>[fredsirieix, lives, peckhamsouth, londons, lo...</td>\n",
       "      <td>[fredsirieix, life, peckhamsouth, london, love...</td>\n",
       "      <td>fredsirieix life peckhamsouth london love gurubig</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text sentiment    j  \\\n",
       "710  11651  maps by the yeah yeah yeahs came on the radio ...     anger  0.0   \n",
       "381  41241  Dreams dashed and divided like million stars i...   sadness  0.0   \n",
       "416  41276  I did the dishes yesterday, fell asleep, woke ...   sadness  0.0   \n",
       "580  41440                nooooo. Poor Blue Bell! not again.    sadness  0.0   \n",
       "146  41006  Are you #serious that #fredsirieix lives in Pe...   sadness  0.0   \n",
       "\n",
       "         s    f      a                                         clean_text  \\\n",
       "710  0.000  0.0  0.292  maps yeah yeah yeahs came radio today burst tears   \n",
       "381  0.688  0.0  0.000  dreams dashed divided like million stars night...   \n",
       "416  0.625  0.0  0.000  dishes yesterday fell asleep woke sink didnt e...   \n",
       "580  0.604  0.0  0.000                              nooooo poor blue bell   \n",
       "146  0.312  0.0  0.000  fredsirieix lives peckhamsouth londons love gu...   \n",
       "\n",
       "                                              tokenize  \\\n",
       "710  [maps, yeah, yeah, yeahs, came, radio, today, ...   \n",
       "381  [dreams, dashed, divided, like, million, stars...   \n",
       "416  [dishes, yesterday, fell, asleep, woke, sink, ...   \n",
       "580                         [nooooo, poor, blue, bell]   \n",
       "146  [fredsirieix, lives, peckhamsouth, londons, lo...   \n",
       "\n",
       "                                   tokenize_lemmatized  \\\n",
       "710  [map, yeah, yeah, yeahs, came, radio, today, b...   \n",
       "381  [dream, dashed, divided, like, million, star, ...   \n",
       "416  [dish, yesterday, fell, asleep, woke, sink, di...   \n",
       "580                         [nooooo, poor, blue, bell]   \n",
       "146  [fredsirieix, life, peckhamsouth, london, love...   \n",
       "\n",
       "                                            final_text  \n",
       "710    map yeah yeah yeahs came radio today burst tear  \n",
       "381   dream dashed divided like million star night sky  \n",
       "416  dish yesterday fell asleep woke sink didnt eat...  \n",
       "580                              nooooo poor blue bell  \n",
       "146  fredsirieix life peckhamsouth london love gurubig  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cc56c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical encoding of our labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_enc = label_encoder.fit_transform(data.sentiment)\n",
    "\n",
    "# one-hot-encode them\n",
    "y = to_categorical(y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "566f1794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-5:]\n",
    "\n",
    "# alphabetical order\n",
    "# 0: anger\n",
    "# 1: fear\n",
    "# 2: joy\n",
    "# 3: sadness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c4015",
   "metadata": {},
   "source": [
    "## Load the tokenizer on which we trained our model\n",
    "If you use another tokenizer it is not going to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f97b8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.json') as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "tokenizer = tokenizer_from_json(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3512e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  15,  303, 1521,  485,  795,  438,  485,  795,  438,  485,  795,\n",
       "         438, 4007, 4008, 4009,  210, 4010,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [  68,  165, 9635,  349,  234,  165,  165, 5317,  339, 3771,  349,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [ 169, 1440,  517,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert texts into some numeric sequences and make the length of all numeric sequences equal \n",
    "\n",
    "X_seq = tokenizer.texts_to_sequences(data.final_text) \n",
    "X_pad = pad_sequences(X_seq, maxlen = 50, padding = 'post') \n",
    "\n",
    "X_pad = np.array(X_pad)\n",
    "X_pad[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01c84fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407    happy birthday seed double exclamation mark do...\n",
       "588    n word normalized medium wow word word frog bo...\n",
       "350                            depressing freaking close\n",
       "Name: final_text, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.final_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32a928f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57b6781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.18835455179214478\n",
      "Test accuracy: 0.931253969669342\n"
     ]
    }
   ],
   "source": [
    "# check external validity on the test set\n",
    "score = model.evaluate(X_pad, y, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e23d9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.81675982e-01, 1.42456861e-02, 2.26224517e-03, 1.81610230e-03],\n",
       "       [9.98395979e-01, 7.80888135e-04, 4.14176786e-04, 4.08927124e-04],\n",
       "       [1.80836429e-03, 1.54076482e-03, 1.18682778e-03, 9.95464027e-01],\n",
       "       ...,\n",
       "       [1.39050861e-03, 1.52586838e-02, 1.79792685e-03, 9.81552899e-01],\n",
       "       [1.36306568e-03, 1.08365493e-03, 1.48600689e-03, 9.96067345e-01],\n",
       "       [3.22623044e-01, 1.03773475e-01, 2.17085972e-01, 3.56517524e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd882d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23360402",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(model.predict(X_pad), axis=1)\n",
    "target = np.argmax(y, axis=1)\n",
    "mistakes = []\n",
    "\n",
    "for i in range(len(X_pad)):\n",
    "    if predictions[i] != target[i]:\n",
    "        mistakes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14ca1f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>j</th>\n",
       "      <th>s</th>\n",
       "      <th>f</th>\n",
       "      <th>a</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>tokenize_lemmatized</th>\n",
       "      <th>final_text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>11348</td>\n",
       "      <td>@RiveraJose39 happy birthday to my seed‼️‼️‼️ ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138</td>\n",
       "      <td>happy birthday seed double exclamation mark do...</td>\n",
       "      <td>[happy, birthday, seed, double, exclamation, m...</td>\n",
       "      <td>[happy, birthday, seed, double, exclamation, m...</td>\n",
       "      <td>happy birthday seed double exclamation mark do...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>11529</td>\n",
       "      <td>Now that the n word is normalized by the media...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509</td>\n",
       "      <td>n word normalized media wow words words frog b...</td>\n",
       "      <td>[n, word, normalized, media, wow, words, words...</td>\n",
       "      <td>[n, word, normalized, medium, wow, word, word,...</td>\n",
       "      <td>n word normalized medium wow word word frog bo...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>41210</td>\n",
       "      <td>@MariamVeiszadeh #depressing it's so freaking ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>depressing freaking close</td>\n",
       "      <td>[depressing, freaking, close]</td>\n",
       "      <td>[depressing, freaking, close]</td>\n",
       "      <td>depressing freaking close</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>11454</td>\n",
       "      <td>@Grace_thomas473 you keep talkin shit and tryi...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.646</td>\n",
       "      <td>talkin shit trying offend youre petty fuck</td>\n",
       "      <td>[talkin, shit, trying, offend, youre, petty, f...</td>\n",
       "      <td>[talkin, shit, trying, offend, youre, petty, f...</td>\n",
       "      <td>talkin shit trying offend youre petty fuck</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>11548</td>\n",
       "      <td>Ultimately, #KeithScott wasn't the man they we...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741</td>\n",
       "      <td>ultimately keithscott wasnt man hand arrest wa...</td>\n",
       "      <td>[ultimately, keithscott, wasnt, man, hand, arr...</td>\n",
       "      <td>[ultimately, keithscott, wasnt, man, hand, arr...</td>\n",
       "      <td>ultimately keithscott wasnt man hand arrest wa...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text sentiment    j  \\\n",
       "407  11348  @RiveraJose39 happy birthday to my seed‼️‼️‼️ ...     anger  0.0   \n",
       "588  11529  Now that the n word is normalized by the media...     anger  0.0   \n",
       "350  41210  @MariamVeiszadeh #depressing it's so freaking ...   sadness  0.0   \n",
       "513  11454  @Grace_thomas473 you keep talkin shit and tryi...     anger  0.0   \n",
       "607  11548  Ultimately, #KeithScott wasn't the man they we...     anger  0.0   \n",
       "\n",
       "         s    f      a                                         clean_text  \\\n",
       "407  0.000  0.0  0.138  happy birthday seed double exclamation mark do...   \n",
       "588  0.000  0.0  0.509  n word normalized media wow words words frog b...   \n",
       "350  0.679  0.0  0.000                          depressing freaking close   \n",
       "513  0.000  0.0  0.646         talkin shit trying offend youre petty fuck   \n",
       "607  0.000  0.0  0.741  ultimately keithscott wasnt man hand arrest wa...   \n",
       "\n",
       "                                              tokenize  \\\n",
       "407  [happy, birthday, seed, double, exclamation, m...   \n",
       "588  [n, word, normalized, media, wow, words, words...   \n",
       "350                      [depressing, freaking, close]   \n",
       "513  [talkin, shit, trying, offend, youre, petty, f...   \n",
       "607  [ultimately, keithscott, wasnt, man, hand, arr...   \n",
       "\n",
       "                                   tokenize_lemmatized  \\\n",
       "407  [happy, birthday, seed, double, exclamation, m...   \n",
       "588  [n, word, normalized, medium, wow, word, word,...   \n",
       "350                      [depressing, freaking, close]   \n",
       "513  [talkin, shit, trying, offend, youre, petty, f...   \n",
       "607  [ultimately, keithscott, wasnt, man, hand, arr...   \n",
       "\n",
       "                                            final_text prediction  \n",
       "407  happy birthday seed double exclamation mark do...      anger  \n",
       "588  n word normalized medium wow word word frog bo...      anger  \n",
       "350                          depressing freaking close    sadness  \n",
       "513         talkin shit trying offend youre petty fuck      anger  \n",
       "607  ultimately keithscott wasnt man hand arrest wa...      anger  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['prediction'] = predictions\n",
    "\n",
    "# alphabetical order\n",
    "# 0: anger\n",
    "# 1: fear\n",
    "# 2: joy\n",
    "# 3: sadness\n",
    "\n",
    "code_to_strings = {0: \"anger\",\n",
    "                   1: \"fear\",\n",
    "                   2: \"joy\",\n",
    "                   3: \"sadness\"}\n",
    "\n",
    "data[\"prediction\"] = data[\"prediction\"].map(code_to_strings)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29b01b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "264270bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3142"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74685534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in predicting all the data we have: 93.13%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy in predicting all the data we have:\", str(round((1-len(mistakes)/len(X_pad))*100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62d93006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "MISTAKEN; didnt bad situation great mate cant believe make despair humanity\n",
      "target: sadness\n",
      "prediction: fear\n",
      "---------------------------------------------------------------------------\n",
      "MISTAKEN; need pout time getting nerve gbbo\n",
      "target: sadness\n",
      "prediction: anger\n",
      "---------------------------------------------------------------------------\n",
      "MISTAKEN; lol dk actually dropped table surprisingly arm warrior dp moment\n",
      "target: fear\n",
      "prediction: joy\n",
      "---------------------------------------------------------------------------\n",
      "MISTAKEN; finger crossed finish work early friday time catch lib frowning face open mouth timetogrind\n",
      "target: fear\n",
      "prediction: anger\n",
      "---------------------------------------------------------------------------\n",
      "MISTAKEN; dreadful franglaise\n",
      "target: sadness\n",
      "prediction: fear\n",
      "---------------------------------------------------------------------------\n",
      "MISTAKEN; internationaldayofpeace white supremacist terrorize differing culture online continue offline tomorrow\n",
      "target: fear\n",
      "prediction: anger\n",
      "---------------------------------------------------------------------------\n",
      "MISTAKEN; feel vigorous fine kill said hestia display despair joyful\n",
      "target: fear\n",
      "prediction: joy\n",
      "---------------------------------------------------------------------------\n",
      "MISTAKEN; wouldnt fret there opportunity life worry working\n",
      "target: sadness\n",
      "prediction: anger\n",
      "---------------------------------------------------------------------------\n",
      "MISTAKEN; ffs dreadful defending\n",
      "target: fear\n",
      "prediction: sadness\n",
      "---------------------------------------------------------------------------\n",
      "MISTAKEN; head melted tired cant sleep\n",
      "target: fear\n",
      "prediction: sadness\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "    print(\"MISTAKEN;\", data['final_text'].values[mistakes[i]])\n",
    "    print(\"target:\", data['sentiment'].values[mistakes[i]])\n",
    "    print(\"prediction:\", data['prediction'].values[mistakes[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981882b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
